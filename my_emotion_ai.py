# -*- coding: utf-8 -*-
"""Copy of Emotion AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dsAozoGEvYuSWvvV4WYp50AiyLjWITVJ

## DETECTOR DE EMOCIONES

# PARTE 1. DETECCIÓN DE PUNTOS FACIALES CLAVES

![alt text](https://drive.google.com/uc?id=1IMU3Ja8JH1Yb4VtEW3t-h90rqYnVaQQZ)

![alt text](https://drive.google.com/uc?id=1FjsnBgcwYthJAzZ-S70tiDaMSPIrjI3L)

IMPORTAR LIBRERÍAS Y DATASETS
"""

# Necesitaremos montar su disco usando los siguientes comandos:
# Para obtener más información sobre el montaje, puedes consultar: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Emotion_AI

!pip install tensorflow==2.8.0
!pip install keras==2.8.0

# Importamos los paquetes necesarios

import pandas as pd
import numpy as np
import os
import PIL
import seaborn as sns
import pickle
from PIL import *
import cv2
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
from IPython.display import display
#from tensorflow.python.keras import * -> PARA LAS NUEVAS VERSIONES DE TF, HACE QUE NO FUNCIONE
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, optimizers
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import *
from tensorflow.keras import backend as K
#from keras import optimizers -> PARA LAS NUEVAS VERSIONES DE TF, HACE QUE NO FUNCIONE
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from google.colab.patches import cv2_imshow

# Cargamos los puntos faciales clave
keyfacial_df = pd.read_csv('/content/drive/My Drive/Curso de Masterclass en IA Moderna/1. Emotion AI/data.csv')

keyfacial_df

# Obtenemos información relevante del dataset
keyfacial_df.info()

# Comprobamos si hay valores nulos en el dataset
keyfacial_df.isnull().sum()

keyfacial_df['Image'].shape

# Dado que los valores para la imagen se dan como cadenas separadas por espacios, separamos los valores usando ' ' como separador.
# Luego convertimos esto en una matriz numérica usando np.fromstring y convierta la matriz 1D obtenida en una matriz 2D de forma (96, 96)
keyfacial_df['Image'] = keyfacial_df['Image'].apply(lambda x: np.fromstring(x, dtype = int, sep = ' ').reshape(96, 96))

# Obtain the Shape of the image
keyfacial_df['Image'][0].shape

keyfacial_df.describe()

"""# VISUALIZACIÓN DE IMÁGENES

"""

# Representamos una imagen aleatoria del conjunto de datos junto con puntos clave faciales.
# Los datos de la imagen se obtienen de df ['Image'] y se representan usando plt.imshow
# 15 coordenadas x e y para la imagen correspondiente
# Dado que las coordenadas x están en columnas pares como 0,2,4, .. y las coordenadas y están en columnas impares como 1,3,5, ..
# Accedemos a su valor usando el comando .loc, que obtiene los valores de las coordenadas de la imagen en función de la columna a la que se refiere.

i = np.random.randint(1, len(keyfacial_df))
plt.imshow(keyfacial_df['Image'][i], cmap = 'gray')
for j in range(1, 31, 2):
        plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], 'rx')

# Veamos más imágenes en formato matricial
fig = plt.figure(figsize=(20, 20))

for i in range(16):
    ax = fig.add_subplot(4, 4, i + 1)
    image = plt.imshow(keyfacial_df['Image'][i],cmap = 'gray')
    for j in range(1,31,2):
        plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], 'rx')

"""
- Realizamos una verificación adicional en los datos visualizando aleatoriamente 64 nuevas imágenes junto con sus puntos clave correspondientes"""

import random

fig = plt.figure(figsize=(20, 20))

for i in range(64):
    k = random.randint(1, len(keyfacial_df))
    ax = fig.add_subplot(8, 8, i + 1)
    image = plt.imshow(keyfacial_df['Image'][k],cmap = 'gray')
    for j in range(1,31,2):
        plt.plot(keyfacial_df.loc[k][j-1], keyfacial_df.loc[k][j], 'rx')

"""# AUMENTACIÓN DE LAS IMÁGENES"""

# Creamos una copia del dataframe
import copy
keyfacial_df_copy = copy.copy(keyfacial_df)

# Obtenemos las columnas del dataframe

columns = keyfacial_df_copy.columns[:-1]
columns

# Horizontal Flip - Damos la vuelta a las imágenes entorno al eje y
keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 1))

# dado que estamos volteando horizontalmente, los valores de la coordenada y serían los mismos
# Solo cambiarían los valores de la coordenada x, todo lo que tenemos que hacer es restar nuestros valores iniciales de la coordenada x del ancho de la imagen (96)
for i in range(len(columns)):
  if i%2 == 0:
    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )

# Mostramos la imagen original
plt.imshow(keyfacial_df['Image'][0], cmap = 'gray')
for j in range(1, 31, 2):
        plt.plot(keyfacial_df.loc[0][j-1], keyfacial_df.loc[0][j], 'rx')

# Mostramos la imagen girada horizontalmente
plt.imshow(keyfacial_df_copy['Image'][0],cmap='gray')
for j in range(1, 31, 2):
        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')

# Concatenamos el dataset original con el dataframe aumentado
augmented_df = np.concatenate((keyfacial_df, keyfacial_df_copy))

augmented_df.shape

# Aumentar aleatoriamente el brillo de las imágenes
# Multiplicamos los valores de los píxeles por valores aleatorios entre 1,5 y 2 para aumentar el brillo de la imagen
# Recortamos el valor entre 0 y 255

import random

keyfacial_df_copy = copy.copy(keyfacial_df)
keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x:np.clip(random.uniform(1.5, 2)* x, 0.0, 255.0))
augmented_df = np.concatenate((augmented_df, keyfacial_df_copy))
augmented_df.shape

# Mostramos la imagen con el brillo aumentado

plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')
for j in range(1, 31, 2):
        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')

keyfacial_df_copy = copy.copy(keyfacial_df)

keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 0))

for i in range(len(columns)):
  if i%2 == 1:
    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )

#Mostramos la imagen volteada
plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')
for j in range(1, 31, 2):
        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')



"""# NORMALIZACIÓN DE LOS DATOS Y PREPARACIÓN PARA EL ENTRENAMIENTO"""

# Obtenemos el valor de las imágenes que está presente en la columna 31 (dado que el índice comienza desde 0, nos referimos a la columna 31 por 30 en Python)
img = augmented_df[:,30]

# Normalizamos las imágenes
img = img/255.

# Creamos un array vacío de tamaño (x, 96, 96, 1) para subministrar al modelo
X = np.empty((len(img), 96, 96, 1))

# Iteramos sobre la lista de imágenes y añadimos las mismas al array vacío tras expandir su dimensión de (96, 96) a (96, 96, 1)
for i in range(len(img)):
  X[i,] = np.expand_dims(img[i], axis = 2)

# Convertimos el tipo array a float32
X = np.asarray(X).astype(np.float32)
X.shape

# Obtenemos el valor de las coordenadas x & y que se utilizarán como target.
y = augmented_df[:,:30]
y = np.asarray(y).astype(np.float32)
y.shape

# Dividimos los datos en entrenamiento y testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train.shape

X_test.shape

"""MINI RETO #7:
- Lista almenos 3 redes neuronales diferentes y sus respectivas aplicaciones.
"""



"""# ENTRENAMIENDO EN REDES Y LOS ALGORITMOS DE GRADIENTE DESCENDENTE

![alt text](https://drive.google.com/uc?id=1_gqDWFp8yxcMORywHD5BpqJdTqzAyfUr)

![alt text](https://drive.google.com/uc?id=1lKbftYl6MTfyNha0GLFQAF56Mbai2xAh)

![alt text](https://drive.google.com/uc?id=1pMTlXBupNYQVb9PGzU0Y1gNpoX18Zyog)

![alt text](https://drive.google.com/uc?id=1HVHA3DTrCjEa2Z6EL5JAHPgilBTTyxA9)

![alt text](https://drive.google.com/uc?id=1DP05EPIqUmRxaTnx4HoaLZGjPZnoRa0N)

# TEORÍA E INTUICIÓN DETRÁS DE LAS REDES NEURONALES CONVOLUCIONALES Y RESNETS

![alt text](https://drive.google.com/uc?id=1MyP2gNs5cGoPpwygKYiQoQoDjRfaNe3L)

![alt text](https://drive.google.com/uc?id=1nVMlRrisFq4_pSxCwm8SVNiANJTmX0-b)

![alt text](https://drive.google.com/uc?id=1we_JB96LvF1r6PC995FNm3M-YzA3fAyI)

![alt text](https://drive.google.com/uc?id=1AkCcwPFXQnpaZKsnePwIIXxCV3Pw6YSw)

# CONSTRUIR UN MODELO DE RED NEURONAL RESIDUAL PROFUNDA PARA CREAR UN MODELO QUE DETECTE PUNTOS FACIALES CLAVE
"""

def res_block(X, filter, stage):

  # Bloque Convolucional
  X_copy = X

  f1 , f2, f3 = filter

  # Camino Principal
  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = MaxPool2D((2,2))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)
  X = Activation('relu')(X)

  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)
  X = Activation('relu')(X)

  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)


  # Camino Corto
  X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)
  X_copy = MaxPool2D((2,2))(X_copy)
  X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)

  # Añadir
  X = Add()([X,X_copy])
  X = Activation('relu')(X)

  # Bloque de Identidad 1
  X_copy = X


  # Camino Principal
  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)
  X = Activation('relu')(X)

  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)
  X = Activation('relu')(X)

  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)

  # Añadir
  X = Add()([X,X_copy])
  X = Activation('relu')(X)

  # Bloque de Identidad 2
  X_copy = X


  # Camino Principal
  X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)
  X = Activation('relu')(X)

  X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)
  X = Activation('relu')(X)

  X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)
  X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)

  # Añadir
  X = Add()([X,X_copy])
  X = Activation('relu')(X)

  return X

input_shape = (96, 96, 1)

# Tamaño del tensor de entrada
X_input = Input(input_shape)

# Zero-padding
X = ZeroPadding2D((3,3))(X_input)

# 1 - Fase
X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = 'bn_conv1')(X)
X = Activation('relu')(X)
X = MaxPooling2D((3,3), strides= (2,2))(X)

# 2 - Fase
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - Fase
X = res_block(X, filter= [128,128,512], stage= 3)

# 4 - Fase
#X = res_block(X, filter= [256,256,1024], stage= 4)

# Average Pooling
X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)

# Capa Final
X = Flatten()(X)
X = Dense(4096, activation = 'relu')(X)
X = Dropout(0.2)(X)
X = Dense(2048, activation = 'relu')(X)
X = Dropout(0.1)(X)
X = Dense(30, activation = 'relu')(X)


model_1_facialKeyPoints = Model( inputs= X_input, outputs = X)
model_1_facialKeyPoints.summary()

"""# COMPILAR Y ENTRENAR EL MODELO DE DEEP LEARNING PARA LA DETECCIÓN DE PUNTOS FACIALES CLAVE"""

adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)
model_1_facialKeyPoints.compile(loss = "mean_squared_error", optimizer = adam , metrics = ['accuracy'])
# Comprueba más sobre el Optimizador Adam en : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam

# Guardamos el mejor modelo con mejor error de validación
checkpointer = ModelCheckpoint(filepath = "FacialKeyPoints_weights.hdf5", verbose = 1,
                               save_best_only = True,  save_weights_only=True)
# necesario añadir  save_weights_only=True en las nuevas versiones de Python

history = model_1_facialKeyPoints.fit(X_train, y_train, batch_size = 32,
                                      epochs = 2, validation_split = 0.05, callbacks=[checkpointer])

# Guardamos la arquitectura del modelo en un JSON para luego usarlo

model_json = model_1_facialKeyPoints.to_json()
with open("FacialKeyPoints-model.json","w") as json_file:
  json_file.write(model_json)

"""# EVALUAR LA EFICACIA DEL MODELO DE DETECCIÓN DE PUNTOS FACIALES CLAVE ENTRENADO"""

with open('detection.json', 'r') as json_file:
    json_savedModel= json_file.read()

# Cargar la arquitectura del modelo
model_1_facialKeyPoints = tf.keras.models.model_from_json(json_savedModel)
model_1_facialKeyPoints.load_weights('weights_keypoint.hdf5')
adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)
model_1_facialKeyPoints.compile(loss="mean_squared_error", optimizer= adam , metrics = ['accuracy'])

# Evaluar el modelo

result = model_1_facialKeyPoints.evaluate(X_test, y_test)
print("Accuracy : {}".format(result[1]))

# Obtenemos las claves del modelo
history.history.keys()

# Representamos los scores del entrenamiento

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train_loss','val_loss'], loc = 'upper right')
plt.show()

"""# PARTE 2. DETECCIÓN DE LAS EXPRESIONES FACIALES

![alt text](https://drive.google.com/uc?id=1yVo-spGYRjUqKgKohd9WDOs1Ws_TKJId)

![alt text](https://drive.google.com/uc?id=1yv_Nm6elYnuNP-tFr2AuIU_kyENEh-Ek)

# IMPORTAR & EXPLORAR EL DATASET PARA LA DETECCIÓN DE EXPRESIONES FACIALES
"""

# Leemos el CSV para los datos de expresiones faciales
facialexpression_df = pd.read_csv('icml_face_data.csv')

facialexpression_df

facialexpression_df[' pixels'][0] # Formato String

# Función para convertir valores de píxel de formato string a formato array

def string2array(x):
  return np.array(x.split(' ')).reshape(48, 48, 1).astype('float32')

# Redimensionamos la imagen de (48, 48) a (96, 96)

def resize(x):

  img = x.reshape(48, 48)
  return cv2.resize(img, dsize=(96, 96), interpolation = cv2.INTER_CUBIC)

facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: string2array(x))

facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: resize(x))

facialexpression_df.head()

# Comprobamos la estructura del data frame
facialexpression_df.shape

# Miramos si hay valores nulos en el data frame
facialexpression_df.isnull().sum()

label_to_text = {0:'Ira', 1:'Odio', 2:'Tristeza', 3:'Felicidad', 4: 'Sorpresa'}

plt.imshow(facialexpression_df[' pixels'][0], cmap = 'gray')

"""# VISUALIZAR LAS IMÁGENES Y MOSTRAR LAS ETIQUETAS"""

emotions = [0, 1, 2, 3, 4]

for i in emotions:
  data = facialexpression_df[facialexpression_df['emotion'] == i][:1]
  img = data[' pixels'].item()
  img = img.reshape(96, 96)
  plt.figure()
  plt.title(label_to_text[i])
  plt.imshow(img, cmap = 'gray')

"""
- Representar un gráfico de barras para averiguar cuántas muestras (imágenes) están presentes por cada emoción"""

facialexpression_df.emotion.value_counts().index

facialexpression_df.emotion.value_counts()

plt.figure(figsize = (10,10))
sns.barplot(x = facialexpression_df.emotion.value_counts().index, y = facialexpression_df.emotion.value_counts())

"""# PREPARACIÓN DE LOS DATOS Y AUMENTACIÓN DE LAS IMÁGENES"""

# Dividir el dataframe en características y etiquetas para la clasificación
from tensorflow.keras.utils import to_categorical

X = facialexpression_df[' pixels']
y = to_categorical(facialexpression_df['emotion'])

X[0]

y

X.shape

X = np.stack(X, axis = 0)
X = X.reshape(24568, 96, 96, 1)

print(X.shape, y.shape)

# Dividir el dataframe en conjunto de entrenamiento, test y validación

from sklearn.model_selection import train_test_split

X_train, X_Test, y_train, y_Test = train_test_split(X, y, test_size = 0.1, shuffle = True)
X_val, X_Test, y_val, y_Test = train_test_split(X_Test, y_Test, test_size = 0.5, shuffle = True)

print(X_val.shape, y_val.shape)

print(X_Test.shape, y_Test.shape)

print(X_train.shape, y_train.shape)

# Pre Procesado de Imágenes

X_train = X_train/255
X_val   = X_val /255
X_Test  = X_Test/255

X_train

train_datagen = ImageDataGenerator(
    rotation_range = 15,
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    shear_range = 0.1,
    zoom_range = 0.1,
    horizontal_flip = True,
    vertical_flip = True,
    brightness_range = [1.1, 1.5],
    fill_mode = "nearest")

"""# CONSTRUIMOS Y ENTRENANOS UN MODELO DE DEEP LEARNING PARA LA CLASIFICACIÓN DE EXPRESIONES FACIALES"""

input_shape = (96, 96, 1)

# Tamaño del tensor de entrada
X_input = Input(input_shape)

# Zero-padding
X = ZeroPadding2D((3, 3))(X_input)

# 1 - Fase
X = Conv2D(64, (7, 7), strides= (2, 2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = 'bn_conv1')(X)
X = Activation('relu')(X)
X = MaxPooling2D((3, 3), strides= (2, 2))(X)

# 2 - Fase
X = res_block(X, filter= [64, 64, 256], stage= 2)

# 3 - Fase
X = res_block(X, filter= [128, 128, 512], stage= 3)

# 4 - Fase
# X = res_block(X, filter= [256, 256, 1024], stage= 4)

# Average Pooling
X = AveragePooling2D((4, 4), name = 'Averagea_Pooling')(X)

# Capa Final
X = Flatten()(X)
X = Dense(5, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)

model_2_emotion = Model(inputs= X_input, outputs = X, name = 'Resnet18')

model_2_emotion.summary()

# Entrenar la red
model_2_emotion.compile(optimizer = "Adam", loss = "categorical_crossentropy", metrics = ["accuracy"])

# Recordemos que el primer modelo de puntos faciales clave se guardó con: FacialKeyPoints_weights.hdf5 and FacialKeyPoints-model.json

# Usamos la parada temprana para salir del entenamiento si el error de validación
# no decrece después de cierto número de epochs (paciencia)
earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)

# Guardamos el mejor modelo con menor error de validación
checkpointer = ModelCheckpoint(filepath = "FacialExpression_weights.hdf5", verbose = 1,
                               save_best_only=True)

history = model_2_emotion.fit(train_datagen.flow(X_train, y_train, batch_size=64),
	validation_data=(X_val, y_val), steps_per_epoch=len(X_train) // 64,
	epochs= 2, callbacks=[ checkpointer, earlystopping])

# Guardar la arquitectura del modelo en un JSON para su futuro uso

model_json = model_2_emotion.to_json()
with open("FacialExpression-model.json","w") as json_file:
  json_file.write(model_json)

"""# EVALUAR LOS MODELOS DE CLASIFICACIÓN (MATRIZ DE CONFUSIÓN, ACIERTO, PRECISIÓN Y RECUPERACIÓN)

![alt text](https://drive.google.com/uc?id=1PLEhcqt2wDz3kb7J8Wu04PjCr46RzzFf)

![alt text](https://drive.google.com/uc?id=1OHdC1j_9pvDxjbIpe74f7uWqyeXx0jBf)

![alt text](https://drive.google.com/uc?id=1GKH-OB9z60Lf_LzTa9xo0MeavZhH3yEV)

# EVALUAR LA EFICACIA DEL MODELO CLASIFICADOR DE EXPRESIONES FACIALES ENTRENADO
"""

with open('emotion.json', 'r') as json_file:
    json_savedModel= json_file.read()

# Cargamos la arquitectura del modelo
model_2_emotion = tf.keras.models.model_from_json(json_savedModel)
model_2_emotion.load_weights('weights_emotions.hdf5')
model_2_emotion.compile(optimizer = "Adam", loss = "categorical_crossentropy", metrics = ["accuracy"])

score = model_2_emotion.evaluate(X_Test, y_Test)
print('Accuracy en la fase de Test: {}'.format(score[1]))

history.history.keys()

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(accuracy))

plt.plot(epochs, accuracy, 'bo', label='Accuracy en el Entrenamiento')
plt.plot(epochs, val_accuracy, 'b', label='Accuracy en la Validación')
plt.title('ACCURACY')
plt.legend()

plt.plot(epochs, loss, 'ro', label='Pérdida en el entrenamiento')
plt.plot(epochs, val_loss, 'r', label='Pérdida en la validación')
plt.title('LOSS')
plt.legend()

# predicted_classes = model.predict_classes(X_test)
predicted_classes = np.argmax(model_2_emotion.predict(X_Test), axis=-1)
y_true = np.argmax(y_Test, axis=-1)

y_true.shape

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, predicted_classes)
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True, cbar = False)

"""
- Mostrar una matriz de 25 imágenes junto con su etiqueta predicha / verdadera
- Mostrar el informe de clasificación y analizar la precisión y la recuperación"""

L = 5
W = 5

fig, axes = plt.subplots(L, W, figsize = (24, 24))
axes = axes.ravel()

for i in np.arange(0, L*W):
    axes[i].imshow(X_test[i].reshape(96,96), cmap = 'gray')
    axes[i].set_title('Predicción = {}\n Verdadera = {}'.format(label_to_text[predicted_classes[i]], label_to_text[y_true[i]]))
    axes[i].axis('off')

plt.subplots_adjust(wspace = 1)

from sklearn.metrics import classification_report
print(classification_report(y_true, predicted_classes))

"""# PARTE 3. COMBINAR LOS MODELOS DE DETECCIÓN DE PUNTOS CLAVE FACIALES Y DE CLASIFICACIÓN DE EXPRESIONES

# COMBINAR AMBOS MODELOS (1) DETECCIÓN DE PUNTOS CLAVE FACIALES Y (2) DE EXPRESIONES FACIALES
"""

def predict(X_test):

  # Hacemos la predicción con el modelo de puntos clave
  df_predict = model_1_facialKeyPoints.predict(X_test)

  # Hacemos la predicción con el modelo de emociones
  df_emotion = np.argmax(model_2_emotion.predict(X_test), axis=-1)

  # Redimensionamos el array de (856,) a (856,1)
  df_emotion = np.expand_dims(df_emotion, axis = 1)

  # Convertimos las predicciones en un dataframe
  df_predict = pd.DataFrame(df_predict, columns= columns)

  # Añadimos la emoción al dataframe de predicciones
  df_predict['emotion'] = df_emotion

  return df_predict

df_predict = predict(X_test)

df_predict.head()

"""
- Representamos una matriz de  16 imágenes junto con su emoción predicha y sus puntos faciales."""

# Representamos las imágenes de test junto con los puntos clave y emociones

fig, axes = plt.subplots(4, 4, figsize = (24, 24))
axes = axes.ravel()

for i in range(16):

    axes[i].imshow(X_test[i].squeeze(),cmap='gray')
    axes[i].set_title('Prediccion = {}'.format(label_to_text[df_predict['emotion'][i]]))
    axes[i].axis('off')
    for j in range(1,31,2):
            axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], 'rx')

"""# PARTE 4. DESPLEGAR LOS DOS MODELOS ENTRENADOS

# GUARDAR EL MODELO ENTRENADO PARA PUBLICAR

![alt text](https://drive.google.com/uc?id=1DBquDj0ce4Vq9KFX7wS-ldT7enXgVGzS)

![alt text](https://drive.google.com/uc?id=1x14nNobS6eeqAjJVZGLZ8MOcgc-WhSBI)

![alt text](https://drive.google.com/uc?id=1zFJZTRkXHWlYQXPQk90IOZnzMTKVKPt5)

- Ahora necesitamos guardar nuestro modelo entrenado y debe guardarse en un formato `SaveModel`.
- El modelo tendrá un número de versión y se guardará en un directorio estructurado
- `tf.saved_model.save` es una función que se usa para crear un modelo guardado que es adecuado para publicar con Tensorflow Serving.
- Una vez guardado el modelo, ahora podemos usar TensorFlow Serving para comenzar a realizar solicitudes de inferencia utilizando una versión específica de nuestro modelo entrenado "servible".
- Utilizaremos `SavedModel` para guardar y cargar nuestro modelo: variables, el gráfico y los metadatos del gráfico.
          
- Consulte esto para obtener más información:
https://www.tensorflow.org/guide/saved_model
"""

import json
import tensorflow.keras.backend as K

def deploy(directory, model):
  MODEL_DIR = directory
  version = 1

  # Juntamos el directorio del temp model con la versión elegida
  # El resultado será = '\tmp\version number'
  export_path = os.path.join(MODEL_DIR, str(version))
  print('export_path = {}\n'.format(export_path))

  # Guardemos el modelo con saved_model.save
  # Si el directorio existe, debemos borrarlo con '!rm'
  # rm elimina cada fichero especificado usando la consola de comandos.

  if os.path.isdir(export_path):
    print('\nAlready saved a model, cleaning up\n')
    !rm -r {export_path}

  tf.saved_model.save(model, export_path)

  os.environ["MODEL_DIR"] = MODEL_DIR

"""# PUBLICAR EL MODELO CON TENSORFLOW SERVING"""

# Agreguemos el paquete tensorflow-model-server a nuestra lista de paquetes
!echo "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \
curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -
!apt update

# Instalemos tensorflow model server
!apt-get install tensorflow-model-server

# Vamos a arrancar TensorFlow serving

"""- Cargaremos nuestro modelo y comenzaremos a hacer inferencias (predicciones) basadas en él.
- Hay algunos parámetros importantes:

   - `rest_api_port`: el puerto que se usará para las solicitudes REST.
   - `model_name`: lo usaremos en la URL de las solicitudes REST. Puedes elegir cualquier nombre
   - `model_base_path`: esta es la ruta al directorio donde ha guardado el modelo.
  
- Para obtener más información sobre REST, consulta en:
https://www.codecademy.com/articles/what-is-rest
- REST es una variante de las llamadas HTTP en el que los comandos http tienen un significado semántico.
"""

deploy('/model', model_1_facialKeyPoints)

# Commented out IPython magic to ensure Python compatibility.
# %%bash --bg
# nohup tensorflow_model_server \
#   --rest_api_port=4500 \
#   --model_name=keypoint_model \
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

!tail server.log

deploy('/model1', model_2_emotion)

# Commented out IPython magic to ensure Python compatibility.
# %%bash --bg
# nohup tensorflow_model_server \
#   --rest_api_port=4000 \
#   --model_name=emotion_model \
#   --model_base_path="${MODEL_DIR}" >server.log 2>&1

!tail server.log

"""- **¡Felicidades! Ahora hemos cargado con éxito una versión servible de nuestro modelo `{name: keypoint_model version: 1}`**
- **¡Felicidades! Ahora hemos cargado con éxito una versión servible de nuestro modelo `{name: emotion_model version: 1}`**

#  HACER PETICIONES AL MODELO CON TENSORFLOW SERVING
"""

import json

# Vamos a crear un objeto JSON y hacer 3 predicciones
data = json.dumps({"signature_name": "serving_default", "instances": X_test[0:3].tolist()})
print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))

!pip install -q requests

import requests

# Función para hacer predicciones con el modelo publicado
def response(data):
  headers = {"content-type": "application/json"}
  json_response = requests.post('http://localhost:4500/v1/models/keypoint_model/versions/1:predict', data=data, headers=headers, verify = False)
  df_predict = json.loads(json_response.text)['predictions']
  json_response = requests.post('http://localhost:4000/v1/models/emotion_model/versions/1:predict', data=data, headers=headers, verify = False)
  df_emotion = np.argmax(json.loads(json_response.text)['predictions'], axis = 1)

  # Redimensión de (856,) a (856,1)
  df_emotion = np.expand_dims(df_emotion, axis = 1)

  # Convertir las predicciones en un dataframe
  df_predict= pd.DataFrame(df_predict, columns = columns)

  # Añadimos la emoción al dataframe de predicciones
  df_predict['emotion'] = df_emotion

  return df_predict

# Hacer una predicción
df_predict = response(data)

df_predict